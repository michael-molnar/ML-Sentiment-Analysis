{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIDI 1002 - AI Algorithms - Final Project\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "Michael Molnar - Durham College #100806823\n",
    "\n",
    "## Notebook 4:  Sentiment Analysis of User Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement:\n",
    "\n",
    "How can we use machine learning to automatically extract the sentiment of every review, comment, blog post, or news article that mentions your business or your products?  \n",
    "\n",
    "This project will create a model that will automatically analyze text and predict its sentiment - negative, neutral, or positive.  This solution will allow a business to automatically parse reviews and comments it receives, sorting them, and allowing for the analyis of how customers feel about the business and brand.  This analysis will allow for a company to determine how feelings towards the company change over time, or after the release of a new product or a shift in direction.  Unhappy customers can be automatically identified and prioritized.  \n",
    "\n",
    "The proposed solution will be a classification model trained on real product reviews to identify the key words and phrases that most accurately predict the sentiment of a sample of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focus of Notebook 4:\n",
    "\n",
    "In the previous notebook I determined that the best model was a Logistic Regression one.  In this notebook I will create the pipeline to take and process user text to match the data the model has been trained and tested on.  I will then recreate the selected model from the previous notebook and create functions to generate sentiments for text input.  \n",
    "\n",
    "The final function will take in the text, do all of the processing and generate the sentiment.  It will produce the probabilities for each of the three classes.  Finally, it will make use of the study done at the end of the last notebook to identify the top five words or phrases that most impacted on the model's class prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Input Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will use all of the text cleaning functions I created in previous notebooks.  All of these will be combined into one function that will transform a user's text into a suitable form to be applied to the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand contractions\n",
    "def decontract(sentence):\n",
    "    sentence = re.sub(r\"won\\'t\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n",
    "    \n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove linebreaks\n",
    "def remove_linebreaks(input):\n",
    "    text = re.compile(r'\\n')\n",
    "    return text.sub(r' ',input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punctuation(input):\n",
    "    no_punc = [char for char in input if char not in string.punctuation]\n",
    "    no_punc = ''.join(no_punc)\n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace numbers with text\n",
    "def replace_numbers(text):\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if word.isdigit():\n",
    "            words.append(num2words(word))\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.remove('no')\n",
    "stopwords_list.remove('not')\n",
    "stopwords_list.remove('very')\n",
    "stopwords_list.remove('only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "def remove_stopwords(input):\n",
    "    no_stop = [word for word in input.split() if word not in stopwords_list]\n",
    "    no_stop = ' '.join(no_stop)\n",
    "    return no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stem text\n",
    "def stem_text(input):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text = input.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "        words += (stemmer.stem(i))+' '\n",
    "    return words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now combine all of these processing techniques into one function that will be used on a user's text.  I note again that I remove punctuation twice - before and after converting numbers to strings - to ensure that any dashes that have been added will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(input):\n",
    "    input = decontract(input)\n",
    "    input = remove_linebreaks(input)\n",
    "    input = remove_punctuation(input)\n",
    "    input = input.lower()\n",
    "    input = replace_numbers(input)\n",
    "    input = remove_punctuation(input)\n",
    "    input = remove_stopwords(input)\n",
    "    input = stem_text(input)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data the Model was Trained and Tested On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('clean_training_data.csv')\n",
    "X_test = pd.read_csv('clean_testing_data.csv')\n",
    "y_train = pd.read_csv('training_labels.csv')\n",
    "y_test = pd.read_csv('testing_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra column\n",
    "X_train.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "X_test.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "y_train.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "y_test.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from dataframes to series\n",
    "X_train = X_train['reviewText']\n",
    "X_test = X_test['reviewText']\n",
    "y_train = y_train['label']\n",
    "y_test = y_test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77271,)\n",
      "(24664,)\n",
      "(77271,)\n",
      "(24664,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Logestic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As noted, I will use a Count Vectorizer with a combination of unigrams, bigrams, and trigrams\n",
    "# This combination produced the highest accuracy in the algorithm testing notebook\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# Vectorization\n",
    "X_train_cv = vectorizer.fit_transform(X_train)\n",
    "X_test_cv = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.974078244101927\n",
      "Testing Accuracy: 0.8607281868310087\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Logistic Regression classifier and checking the accuracies\n",
    "lr = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "lr.fit(X_train_cv, y_train) \n",
    "lr_train_preds = lr.predict(X_train_cv)\n",
    "lr_preds = lr.predict(X_test_cv)\n",
    "lr_train_acc = accuracy_score(y_train, lr_train_preds)\n",
    "lr_test_acc = accuracy_score(y_test, lr_preds)\n",
    "print(\"Training Accuracy:\", lr_train_acc)\n",
    "print(\"Testing Accuracy:\", lr_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match the Vectorizer Features and the Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will repeat the process from the last notebook of matching the features in the vectorizer with the model's coefficients for each of the three classes.  This will allow me to match the words and phrases found in the input text to these coefficients and then rank them according to their importance to the prediction of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1753074)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1753074"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients for the negative class\n",
    "negative_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        vectorizer.get_feature_names(), lr.coef_[0])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients for the neutral class\n",
    "neutral_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        vectorizer.get_feature_names(), lr.coef_[1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients for the positive class\n",
    "positive_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        vectorizer.get_feature_names(), lr.coef_[2])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions for New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a simplified function.  It will allow the user to enter their text and then process and vectorize it.  \n",
    "It will then predict the sentiment according to the Logistic Regression model that has been fit.  \n",
    "It will then print the probabilities assigned to each of the three classes.\n",
    "\n",
    "\"\"\"\n",
    "def predict_sentiment():\n",
    "    # Prompt the user for text\n",
    "    user_text = input('Enter Your Text Here: \\n\\n')\n",
    "    # Process \n",
    "    processed_text = [process_input(user_text)]\n",
    "    # Transform the text according to the Count Vectorizer that has been fit\n",
    "    vec_text = vectorizer.transform(processed_text)\n",
    "    \n",
    "    # Predict the class label\n",
    "    predicted_sentiment = lr.predict(vec_text)\n",
    "    # Generate the probabilities for the labels\n",
    "    confidence = lr.predict_proba(vec_text)\n",
    "           \n",
    "    # Print a summary\n",
    "    print('\\n\\nThe Predicted Sentiment is: ', predicted_sentiment[0].upper())\n",
    "    print('\\nAnalysis:')\n",
    "    print('\\nProbability of Negative Label:', np.around(confidence[0][0]*100, 2), '%')\n",
    "    print('Probability of Neutral Label:', np.around(confidence[0][1]*100, 2), '%')\n",
    "    print('Probabilty of Positive Label:', np.around(confidence[0][2]*100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions and Analysis for New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function extends the previous one.  After the class label is predicted it will \n",
    "create a list of the vectorized features of the input.  It will then create a dictionary\n",
    "of the model's coefficients for these features based on the label.  Finally, it will sort \n",
    "these values and print out a dataframe of the five with the highest importance. \n",
    "\n",
    "\"\"\"\n",
    "def predict_sentiment_with_analysis():\n",
    "    # Prompt the user for text\n",
    "    user_text = input('Enter Your Text Here: \\n\\n')\n",
    "    # Process\n",
    "    processed_text = [process_input(user_text)]\n",
    "    # Transform the text according to the Count Vectorizer that has been fit\n",
    "    vec_text = vectorizer.transform(processed_text)\n",
    "    \n",
    "    # Predict the class label\n",
    "    predicted_sentiment = lr.predict(vec_text)\n",
    "    # Generate the probabilities for the labels\n",
    "    confidence = lr.predict_proba(vec_text)\n",
    "    \n",
    "    # Get the features of the input text and store the English words or phrases\n",
    "    phrases = []\n",
    "    for item in vec_text.indices:\n",
    "        phrases.append(vectorizer.get_feature_names()[item])\n",
    "    \n",
    "    # Create a dictionary of the model's coefficients for these features \n",
    "    importances = dict()\n",
    "    for phrase in phrases:\n",
    "        if predicted_sentiment == 'negative':\n",
    "            importances[phrase] = negative_coef.get(phrase)\n",
    "        elif predicted_sentiment == 'neutral':\n",
    "            importances[phrase] = neutral_coef.get(phrase)\n",
    "        elif predicted_sentiment == 'positive':\n",
    "            importances[phrase] = positive_coef.get(phrase)\n",
    "            \n",
    "    # Sort this dictionary according to its values\n",
    "    importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    # Create a dataframe of these features and \n",
    "    importances = pd.DataFrame(importances)\n",
    "    importances = importances[:5]\n",
    "    importances = importances.rename(columns={0: 'Word or Phrase', 1: 'Model Coefficient'},\n",
    "                                        index={0: 'First', 1: 'Second', 2: 'Third', 3: 'Fourth', 4: 'Fifth'})\n",
    "        \n",
    "    # Print the results\n",
    "    print('\\n\\nThe Predicted Sentiment is: ', predicted_sentiment[0].upper())\n",
    "    print('\\nAnalysis:')\n",
    "    print('\\nProbability of Negative Label:', np.around(confidence[0][0]*100, 2), '%')\n",
    "    print('Probability of Neutral Label:', np.around(confidence[0][1]*100, 2), '%')\n",
    "    print('Probabilty of Positive Label:', np.around(confidence[0][2]*100, 2), '%')\n",
    "    print('\\nThe Five Most Important Stemmed Words or Phrases Are: \\n')\n",
    "    print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model on Real Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have searched on Amazon for one positive, one negative, and one neutral review and I will examine the results of my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Source:</b>\n",
    "\n",
    "https://www.amazon.ca/product-reviews/B07XP1CNRW/ref=acr_dp_hist_5?ie=UTF8&filterByStar=five_star&reviewerType=all_reviews#reviews-filter-bar\n",
    "\n",
    "<b>Rating:</b>\n",
    "\n",
    "5/5\n",
    "\n",
    "<b>Text:</b>\n",
    "\n",
    "I was looking to get back into painting and these paints were incredible! I was unsure of whether to purchase or not because they were much more affordable than some other options I saw but I am so glad I purchased these. Whether these are for children or for yourself, you can enjoy and you really get the feel that they are high quality. I've worked with expensive acrylic paints before and these are very similar. They blend very seamlessly together, and the colour selection is absolutely gorgeous. The paint tubes are actually a decent size so you have good value there as well. None of the paints were dried up or anything, and so far they have exceeded my expectations. They arrived in very nice packaging as well so I would imagine that it would make a great gift. Highly recommend this!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Text Here: \n",
      "\n",
      "I was looking to get back into painting and these paints were incredible! I was unsure of whether to purchase or not because they were much more affordable than some other options I saw but I am so glad I purchased these. Whether these are for children or for yourself, you can enjoy and you really get the feel that they are high quality. I've worked with expensive acrylic paints before and these are very similar. They blend very seamlessly together, and the colour selection is absolutely gorgeous. The paint tubes are actually a decent size so you have good value there as well. None of the paints were dried up or anything, and so far they have exceeded my expectations. They arrived in very nice packaging as well so I would imagine that it would make a great gift. Highly recommend this!!\n",
      "\n",
      "\n",
      "The Predicted Sentiment is:  POSITIVE\n",
      "\n",
      "Analysis:\n",
      "\n",
      "Probability of Negative Label: 0.0 %\n",
      "Probability of Neutral Label: 0.13 %\n",
      "Probabilty of Positive Label: 99.87 %\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Text Here: \n",
      "\n",
      "I was looking to get back into painting and these paints were incredible! I was unsure of whether to purchase or not because they were much more affordable than some other options I saw but I am so glad I purchased these. Whether these are for children or for yourself, you can enjoy and you really get the feel that they are high quality. I've worked with expensive acrylic paints before and these are very similar. They blend very seamlessly together, and the colour selection is absolutely gorgeous. The paint tubes are actually a decent size so you have good value there as well. None of the paints were dried up or anything, and so far they have exceeded my expectations. They arrived in very nice packaging as well so I would imagine that it would make a great gift. Highly recommend this!!\n",
      "\n",
      "\n",
      "The Predicted Sentiment is:  POSITIVE\n",
      "\n",
      "Analysis:\n",
      "\n",
      "Probability of Negative Label: 0.0 %\n",
      "Probability of Neutral Label: 0.13 %\n",
      "Probabilty of Positive Label: 99.87 %\n",
      "\n",
      "The Five Most Important Stemmed Words or Phrases Are: \n",
      "\n",
      "       Word or Phrase  Model Coefficient\n",
      "First           great           2.518729\n",
      "Second           glad           1.994195\n",
      "Third        gorgeous           1.860844\n",
      "Fourth          enjoy           1.394096\n",
      "Fifth            nice           1.380592\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment_with_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is almost certain of the positive sentiment here.  The review contains some very telling words, \"great\", \"glad\", and \"gorgeous\", being the top three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neutral Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Source:</b>\n",
    "\n",
    "https://www.amazon.ca/product-reviews/B076KCGRS6/ref=?ie=UTF8&filterByStar=three_star&reviewerType=all_reviews&pageNumber=1#reviews-filter-bar\n",
    "\n",
    "<b>Rating:</b>\n",
    "\n",
    "3/5\n",
    "\n",
    "<b>Text:</b>\n",
    "\n",
    "These pencils are pretty good. They're not amazing but they're good enough for beginners at a decent price. The leads are quite strong. I've been using them for a few weeks now and I haven't had one break yet. What bothers me is that they aren't labeled, so it can be hard to tell which color is which. I often have to hold it up into the light to tell the difference, and I still have a hard time since some of them look so similar. I'll usually just scribble it on a separate piece of paper beforehand, just to be sure I have the correct pencil. In the future, I plan on labeling them myself to avoid wasting so much time. The variety of colors are okay, but they're not the most \"natural\" of hues (think comic book colors) I had to go out and purchase additional colors separately (because I needed more pastel colors) Artists might need a different gray pencil because it looks more like a sandy beige...and the white pencil does absolutely nothing. Overall I am happy with this purchase...but as they run out individually, I will probably replace each of them one by one with different brands, just so I can choose the exact colors I need for specific projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Text Here: \n",
      "\n",
      "These pencils are pretty good. They're not amazing but they're good enough for beginners at a decent price. The leads are quite strong. I've been using them for a few weeks now and I haven't had one break yet. What bothers me is that they aren't labeled, so it can be hard to tell which color is which. I often have to hold it up into the light to tell the difference, and I still have a hard time since some of them look so similar. I'll usually just scribble it on a separate piece of paper beforehand, just to be sure I have the correct pencil. In the future, I plan on labeling them myself to avoid wasting so much time. The variety of colors are okay, but they're not the most \"natural\" of hues (think comic book colors) I had to go out and purchase additional colors separately (because I needed more pastel colors) Artists might need a different gray pencil because it looks more like a sandy beige...and the white pencil does absolutely nothing. Overall I am happy with this purchase...but as they run out individually, I will probably replace each of them one by one with different brands, just so I can choose the exact colors I need for specific projects.\n",
      "\n",
      "\n",
      "The Predicted Sentiment is:  NEUTRAL\n",
      "\n",
      "Analysis:\n",
      "\n",
      "Probability of Negative Label: 0.0 %\n",
      "Probability of Neutral Label: 99.99 %\n",
      "Probabilty of Positive Label: 0.01 %\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Text Here: \n",
      "\n",
      "These pencils are pretty good. They're not amazing but they're good enough for beginners at a decent price. The leads are quite strong. I've been using them for a few weeks now and I haven't had one break yet. What bothers me is that they aren't labeled, so it can be hard to tell which color is which. I often have to hold it up into the light to tell the difference, and I still have a hard time since some of them look so similar. I'll usually just scribble it on a separate piece of paper beforehand, just to be sure I have the correct pencil. In the future, I plan on labeling them myself to avoid wasting so much time. The variety of colors are okay, but they're not the most \"natural\" of hues (think comic book colors) I had to go out and purchase additional colors separately (because I needed more pastel colors) Artists might need a different gray pencil because it looks more like a sandy beige...and the white pencil does absolutely nothing. Overall I am happy with this purchase...but as they run out individually, I will probably replace each of them one by one with different brands, just so I can choose the exact colors I need for specific projects.\n",
      "\n",
      "\n",
      "The Predicted Sentiment is:  NEUTRAL\n",
      "\n",
      "Analysis:\n",
      "\n",
      "Probability of Negative Label: 0.0 %\n",
      "Probability of Neutral Label: 99.99 %\n",
      "Probabilty of Positive Label: 0.01 %\n",
      "\n",
      "The Five Most Important Stemmed Words or Phrases Are: \n",
      "\n",
      "       Word or Phrase  Model Coefficient\n",
      "First            okay           2.350509\n",
      "Second         decent           1.185844\n",
      "Third       hard time           0.874930\n",
      "Fourth        probabl           0.758386\n",
      "Fifth     good enough           0.607285\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment_with_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is almost certain of the neutral class.  It has relied most on the words \"okay\" and \"decent\" in making the determination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model was so sure of the first two labels for this one I have selected a review that was rated one out of five but does not contain any extremely negative words or phrases - there is no \"terrible\", \"horrible\", \"waste of money\", or other such phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Source:</b>\n",
    "\n",
    "https://www.amazon.ca/product-reviews/B076KCGRS6/ref=cm_cr_unknown?ie=UTF8&filterByStar=one_star&reviewerType=all_reviews&pageNumber=1#reviews-filter-bar\n",
    "\n",
    "<b>Rating:</b>\n",
    "\n",
    "1/5\n",
    "\n",
    "<b>Text:</b>\n",
    "\n",
    "We ordered the full 3 sets and before we knew it they were breaking constantly. Thinking it was our sharpener, we used a different one and the problem persisted. You will get breakages but not losing 50mm in less than a hour. Before long that pencil will be gone.\n",
    "We returned ours for a full refund and bought some Castles, much better.\n",
    "Don’t know how these are constructed, but it looks like a 2 piece outer with pressured stain, which disguises the grain whether good or bad.\n",
    "We contacted the seller who was very sympathetic and sent us a free box of Cobras, but sadly no different, actually these were worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Text Here: \n",
      "\n",
      "We ordered the full 3 sets and before we knew it they were breaking constantly. Thinking it was our sharpener, we used a different one and the problem persisted. You will get breakages but not losing 50mm in less than a hour. Before long that pencil will be gone. We returned ours for a full refund and bought some Castles, much better. Don’t know how these are constructed, but it looks like a 2 piece outer with pressured stain, which disguises the grain whether good or bad. We contacted the seller who was very sympathetic and sent us a free box of Cobras, but sadly no different, actually these were worse.\n",
      "\n",
      "\n",
      "The Predicted Sentiment is:  NEGATIVE\n",
      "\n",
      "Analysis:\n",
      "\n",
      "Probability of Negative Label: 82.56 %\n",
      "Probability of Neutral Label: 17.44 %\n",
      "Probabilty of Positive Label: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Text Here: \n",
      "\n",
      "We ordered the full 3 sets and before we knew it they were breaking constantly. Thinking it was our sharpener, we used a different one and the problem persisted. You will get breakages but not losing 50mm in less than a hour. Before long that pencil will be gone. We returned ours for a full refund and bought some Castles, much better. Don’t know how these are constructed, but it looks like a 2 piece outer with pressured stain, which disguises the grain whether good or bad. We contacted the seller who was very sympathetic and sent us a free box of Cobras, but sadly no different, actually these were worse.\n",
      "\n",
      "\n",
      "The Predicted Sentiment is:  NEGATIVE\n",
      "\n",
      "Analysis:\n",
      "\n",
      "Probability of Negative Label: 82.56 %\n",
      "Probability of Neutral Label: 17.44 %\n",
      "Probabilty of Positive Label: 0.0 %\n",
      "\n",
      "The Five Most Important Stemmed Words or Phrases Are: \n",
      "\n",
      "       Word or Phrase  Model Coefficient\n",
      "First          return           1.895161\n",
      "Second            bad           1.473265\n",
      "Third            sent           1.338053\n",
      "Fourth           wors           1.110683\n",
      "Fifth             sad           1.070498\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment_with_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this negative review the coefficients are not as large as with the two previous tests.  This is becuase the text does not contain any of the most predictive negative words and phrases that were identified in the last notebook.  Still, the model is over 82% sure that this review is negative, and gives the words it finds most predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is performing very well on user text.  So far this project has comprised of an EDA of the Arts, Crafts, and Sewing subset of the Amazon Product Reviews dataset, in which common words and phrases were identified.  Functions have been written to clean and process text.  This involved expanding contractions, converting numbers to text, removing punctuation, linebreaks and stop words, and then stemming the text.  Six machine learning classifiers have examined and tested using a Count Vectorizer and unigrams, bigrams, trigrams, and combinations.  In the end a Logistic Regression classifier was chosen and tuned, coupled with a Count Vectorizer that used a combination of unigrams, bigrams, and trigrams.  Finally, a pipeline was created to process and vectorize input text in order to use the model to predict its sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Cloud deployment of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
